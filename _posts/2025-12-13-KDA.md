---
layout: post
title: "Linear Attention: Kimi Delta Attention"
date: 2025-12-13
categories: [Attention]
tags: [Attention]
---

Reading the following paper:
- [Kimi Linear: An Expressive, Efficient Attention Architecture](https://arxiv.org/pdf/2510.26692)

As LLM move toward agentic workflows and reinforcement learning (RL) scaling, the $O(T^2)$ complexity and linear Key-Value (KV) cache growth of standard Softmax attention have become prohibitive bottlenecks. While linear attention offers $O(T)$ efficiency, it has historically failed to match the recall and "copying" capabilities of full attention.

**Kimi Linear** introduces **Kimi Delta Attention (KDA)**, a hardware-optimized linear module that utilizes fine-grained gating. By interleaving KDA with standard Multi-Head Latent Attention (MLA) in a **3:1 ratio**, the model achieves superior performance across short, long, and RL benchmarks while reducing KV cache usage by up to **75%** and increasing decoding throughput by **6$\times$** at 1M context lengths.

---

### Technical Deep Dive: Kimi Delta Attention (KDA)

The core contribution is KDA, which improves upon the "Gated DeltaNet" (GDN) and Mamba architectures.

#### A. The Math: Fine-Grained Decay
Standard linear attention (like standard DeltaNet or Mamba2) often uses coarse, head-wise gating. KDA introduces **channel-wise** fine-grained decay. The state update rule is:
$$S_t = (I - \beta_t k_t k_t^\top) \text{Diag}(\alpha_t) S_{t-1} + \beta_t k_t v_t^\top$$
Here, $\text{Diag}(\alpha_t)$ allows each feature dimension to maintain an independent forgetting rate, offering precise control over the finite-state memory. This is critical for the "associative recall" and "copying" tasks where linear attention usually struggles.

#### B. Hardware Efficiency vs. DPLR
KDA is a specialized variant of the **Diagonal-Plus-Low-Rank (DPLR)** recurrence ($S_t = (D - ab^\top)S_{t-1} + \dots$).
*   **The Bottleneck:** General DPLR is computationally expensive and difficult to parallelize efficiently due to numerical precision issues that require secondary chunking.
*   **The Kimi Solution:** KDA binds the DPLR low-rank vectors $a$ and $b$ to the key vector $k$ (i.e., $a_t = \beta_t k_t$ and $b_t = k_t \odot \alpha_t$). This constraint eliminates redundant matrix multiplications and secondary chunking steps, improving operator efficiency by roughly **100%** compared to general DPLR formulations.

#### C. Positional Encoding as Data-Dependent Decay
KDA functions as a **learnable, data-dependent Positional Encoding**.
*   Standard RoPE rotates keys/queries based on fixed frequencies to encode relative distance.
*   KDA’s decay mechanism ($\alpha_t$) acts similarly to a multiplicative positional encoding but relaxes the orthogonality constraint of RoPE. This allows the model to dynamically decide the "distance" or relevance of past tokens based on data content.

---

### Architecture Design: The Hybrid Strategy

The Kimi Linear model is not purely linear; it is a **Hybrid MoE** (Mixture-of-Experts) architecture.

*   **Hybrid Ratio:** The model interleaves **3 KDA layers** for every **1 Full Attention (MLA)** layer. Ablation studies showed this provided the optimal trade-off between throughput and validation loss (better than 1:1 or 7:1).
*   **NoPE (No Positional Encoding) for MLA:** The global attention layers (MLA) do *not* use RoPE. The model delegates all positional awareness and recency bias to the KDA layers. This simplifies long-context training and eliminates the need for frequency base tuning (e.g., YaRN) when extending context windows.
*   **Specs:** The primary evaluated model has 48B total parameters (3B activated), utilizing 8 out of 64 experts.

---

### Evaluation

Comparisons against Full Attention (MLA) and Hybrid Gated DeltaNet (GDN-H) baselines using identical training recipes (1.4T to 5.7T tokens).

*   **Synthetic Tasks:** KDA solved the "Palindrome" and "Associative Recall" tasks perfectly up to 2048 tokens, whereas standard Mamba2 failed, proving the necessity of the Delta rule + fine-grained decay.
*   **General Performance:** Kimi Linear outperformed the full-attention MLA baseline on MMLU, GSM8K, and coding benchmarks.
*   **Long Context (128k+):** On the RULER benchmark, Kimi Linear achieved a score of **84.3** (vs. 81.3 for MLA), showing that the hybrid design handles long-range dependencies better than pure full attention.
*   **RL Scaling:** In Reinforcement Learning runs (math domain), Kimi Linear showed faster convergence and higher asymptotic accuracy than MLA, validating its suitability for "test-time scaling".

---

### Efficiency Gains

The shift to Kimi Linear yields massive computational dividends:
*   **Prefilling:** Matches or slightly exceeds MLA speed at short contexts, but becomes **2.9$\times$ faster** at 1M tokens.
*   **Decoding:** Achieves **6$\times$ higher throughput** (Time Per Output Token) at 1M context compared to MLA.
*   **Memory:** Reduces KV cache memory footprint by **75%**, allowing for significantly larger batch sizes during inference.

---

### Insight

"Positional Encoding" and "Memory Management" are two sides of the same coin. By treating the decay matrix in linear attention as a dynamic position encoding, it removed the need for explicit RoPE in the global layers. This is a significant simplification that likely contributes to the model's stability in extrapolation.

Think of standard Full Attention (MLA) as a **lossless `.bmp` image** of the conversation history—it keeps every pixel (token pair) perfect but becomes massive and slow to process as the image gets larger.
Think of standard Linear Attention as a **`.jpeg` compression**—it is fast and small, but it gets "blurry" (lossy) and loses specific details (like exact phone numbers or code snippets).
**Kimi Linear** acts like a **smart `.zip` archive**:
1.  **KDA** uses "fine-grained decay" to selectively compress the background noise while keeping the important bits in high fidelity (the linear layers).
2.  Periodically, it opens a full "window" (the 1 MLA layer) to check the raw data, ensuring nothing was lost.
3.  Because it compresses efficiently, it can read a "1-million-page book" (1M context) 6 times faster than the model trying to hold every single page in memory at once.
