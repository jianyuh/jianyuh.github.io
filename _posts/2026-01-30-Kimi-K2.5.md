---
layout: post
title: "Kimi K2.5 Reading Note"
date: 2026-01-30
categories: [Kimi]
tags: [Kimi]
---

Reading the following paper:
- [Kimi K2.5: Visual Agentic Intelligence](https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf)


Kimi K2.5 is a multimodal agentic model designed to bridge the gap between vision-language models (VLMs) and autonomous agents. Built upon the Kimi K2 Mixture-of-Experts (MoE) architecture, K2.5 introduces two primary technical innovations: **native multimodal joint optimization** (enhancing both text and vision through co-training) and **Agent Swarm**, a framework for parallel agent orchestration. The model demonstrates state-of-the-art performance across reasoning, coding, and video understanding benchmarks, with the Agent Swarm framework reducing inference latency by up to 4.5x compared to sequential baselines.

---

### **1. Model Architecture & Infrastructure**
*   **Backbone:** The model is based on Kimi K2, a 1.04 trillion parameter MoE (32B activated parameters per token) trained on 15 trillion tokens.
*   **Vision Encoder (MoonViT-3D):**
    *   Utilizes the **NaViT packing strategy**, allowing variable-resolution image inputs without complex splitting.
    *   Implements a **lightweight 3D ViT compression** mechanism where consecutive frames are grouped in fours and temporally averaged at the patch level. This provides 4x temporal compression, enabling the processing of videos up to 2,000+ frames within the context window.
*   **Decoupled Encoder Process (DEP):** To address load imbalances caused by variable multimodal inputs (e.g., varying image counts), the training infrastructure decouples the vision encoder from the main backbone. This allows the system to maintain 90% of the training efficiency of pure text training.

### **2. Key Technical Innovations in Training**

#### **A. Native Multimodal Pre-Training (Counter-Intuitive Finding)**
Contrary to the prevailing "late fusion" consensus (where vision tokens are added at high ratios late in training), K2.5 employs **early fusion with lower vision ratios**. Ablation studies revealed that introducing vision tokens early at a moderate ratio (e.g., 10-20%) yields better multimodal and textual performance under a fixed token budget than aggressive late-stage injection, avoiding the "representation shock" that disrupts linguistic capabilities.

#### **B. Zero-Vision SFT**
During supervised fine-tuning (SFT), the team discovered that **text-only SFT** effectively activates visual tool-use capabilities.
*   **Method:** Image manipulations are proxied through programmatic operations (e.g., IPython) in text data.
*   **Insight:** Adding human-designed visual trajectories during SFT actually hurt generalization. Text-only SFT acts as a "cold start" mechanism, likely because the joint pre-training has already established strong alignment. This allows the model to perform pixel-level operations (like counting or binarization) without explicit vision-SFT data.

#### **C. Joint Multimodal Reinforcement Learning (RL)**
The model utilizes a Unified Agentic RL environment with **Generative Reward Models (GRMs)** that evaluate trajectories based on values like helpfulness and strict instruction following, rather than just binary outcomes.
*   **Cross-Modal Transfer:** A critical finding is that **visual RL enhances textual performance**. After visual RL, benchmarks like MMLU-Pro and GPQA-Diamond saw improvements (+1.7% and +2.1% respectively), suggesting that visual grounding reduces uncertainty in complex reasoning tasks.
*   **Toggle Mechanism:** To prevent "length-overfitting" (where models fail to generalize to higher compute scales), the training alternates between budget-constrained optimization and standard inference-time scaling phases. This reduces output tokens by 25-30% without performance degradation.

### **3. Agent Swarm: Parallel Orchestration Framework**
To solve the linear latency scaling of sequential agents, K2.5 introduces **Agent Swarm**, a Parallel-Agent Reinforcement Learning (PARL) framework.

*   **Architecture:** A trainable **Orchestrator** dynamically instantiates and schedules **frozen sub-agents**.
    *   *Decoupling:* Sub-agents are frozen during training to solve credit assignment ambiguity and training instability.
    *   *Context Management:* Sub-agents maintain local contexts, returning only relevant results to the orchestrator. This acts as a "proactive context management" system, superior to reactive truncation strategies like "Discard-all".
*   **Reward Formulation:** The PARL reward includes specific terms to guide behavior:
    *   $r_{parallel}$: Incentivizes concurrent scheduling to prevent "serial collapse" (defaulting to single-agent mode).
    *   $r_{finish}$: Prevents "spurious parallelism" (creating useless sub-agents to hack the parallelism reward).
*   **Critical Steps Metric:** Optimization focuses on minimizing **Critical Steps**—defined by the longest path in the execution graph (time taken by the main agent + the longest-running parallel sub-agent)—rather than total steps.

### **4. Evaluation Highlights**
*   **Reasoning:** K2.5 achieves 96.1% on **AIME 2025** and 87.1% on **MMLU-Pro**, performing competitively with GPT-5.2 and Claude Opus 4.5.
*   **Coding:** Scores 76.8% on **SWE-Bench Verified** and 85.0% on **LiveCodeBench**, demonstrating robustness in real-world software engineering.
*   **Video Understanding:** Establishes new SOTA results on **VideoMMMU** (86.6%) and **LongVideoBench** (79.8%), driven by the MoonViT-3D architecture.
*   **Agentic Search:** The Agent Swarm framework shines here, achieving **78.4% on BrowseComp** (vs. 60.6% for single-agent K2.5) and reducing execution time by 4.5x on WideSearch tasks.

### **Takeaways**
The most significant takeaway from Kimi K2.5 is the **successful inversion of standard VLM training wisdom**. By validating that *early, low-ratio vision fusion* outperforms late fusion, and that *text-only SFT* can bootstrap visual agents better than visual SFT, strong cross-modal alignment is foundational rather than additive. Furthermore, the **Agent Swarm** represents a practical leap in deploying agents for heavy workloads (e.g., 40GB video analysis), moving from "chain-of-thought" to **"graph-of-agents"** where latency is managed via learned parallelism rather than heuristics.
